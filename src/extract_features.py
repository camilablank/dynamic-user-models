#!/usr/bin/env python3
"""
One-pass feature extractor for dialogs generated by generate_llm_user_model_dataset.py

â€¢ Per attribute (emotion | confidence | trust | knowledge):
   - For each USER turn, build transcript up to that turn + probe suffix:
         "Assistant: I think the {attribute} of this user is"
   - Run the model ONCE with output_hidden_states=True (batched).
   - On-GPU slice the last-token vector for EVERY transformer layer.
   - Stream per-layer features to disk:
         <out_dir>/<attr>_L<layer>.npz

Extras for 'knowledge':
   - Optional fact indexing: for each USER-turn example, record which
     meta.knowledge_facts entry is referenced (best match) via --knowledge_fact_index on.
     Saved as 'kfact_idx' (int32, -1 if none). The facts table is stored in info.

Each per-layer NPZ contains:
  - X: float16/float32 [N, hidden]  (last-token features)
  - y: object          [N]          (normalized labels per USER turn)
  - metas: object      [N,2]        ((conv_id, user_turn_idx))
  - kfact_idx: int32   [N]          (optional; knowledge only)
  - info: dict(model, hidden_size, layer_index, prompt_suffix, max_seq_len, batch_size, save_dtype, knowledge_facts?)
"""

import argparse
import json
import os
import re
from typing import Any, Dict, List, Tuple, Sequence, Optional

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm

# ---------------- Dataset compatibility ----------------
ATTRS = ["emotion", "confidence", "trust", "knowledge"]
PROMPT_SUFFIX_FMT = "Assistant: I think the {attribute} of this user is"

USER_PREFIX = "User: "
ASSIST_PREFIX = "Assistant: "

# Label normalization to keep probe targets tidy
NORM = {
    "emotion": {
        "joyful":"joyful","content":"content","neutral":"neutral",
        "anxious":"anxious","frustrated":"frustrated","sad":"sad","angry":"angry","hopeful":"hopeful",
        # loose mappings
        "happy":"joyful","ok":"neutral","calm":"neutral","upset":"angry","mad":"angry"
    },
    "confidence": {
        "very_low":"very_low","low":"low","medium":"medium","high":"high","very_high":"very_high",
        # loose
        "unsure":"low","uncertain":"low","confident":"high","neutral":"medium"
    },
    "trust": {
        "distrustful":"distrustful","wary":"wary","neutral":"neutral","trusting":"trusting","overtrusting":"overtrusting",
        # loose
        "skeptical":"wary","neutral_trust":"neutral","open":"trusting"
    },
    "knowledge": {
        "knows":"knows","does_not_know":"does_not_know",
        # loose
        "unknown":"does_not_know","unsure":"does_not_know","certain":"knows","not_sure":"does_not_know"
    }
}
FALLBACK = {"emotion":"neutral","confidence":"medium","trust":"neutral","knowledge":"does_not_know"}


# ---------------- IO helpers ----------------
def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def load_jsonl(path: str) -> List[Dict[str, Any]]:
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                rows.append(json.loads(line))
    return rows

def append_probe(prompt_so_far: str, attribute: str) -> str:
    suf = PROMPT_SUFFIX_FMT.format(attribute=attribute)
    sep = "" if (not prompt_so_far or prompt_so_far.endswith("\n")) else "\n"
    return f"{prompt_so_far}{sep}{suf}"

def normalize_turn_label(attr: str, raw: str) -> str:
    raw = (raw or "").strip().lower()
    return NORM.get(attr, {}).get(raw, FALLBACK[attr])


# ---------------- Knowledge fact matching ----------------
_word_re = re.compile(r"\w+")

def _tokenize(s: str) -> List[str]:
    return _word_re.findall(s.lower())

def _loose_overlap(a: str, b: str) -> float:
    ta, tb = _tokenize(a), _tokenize(b)
    if not ta or not tb:
        return 0.0
    sa, sb = set(ta), set(tb)
    inter = len(sa & sb)
    denom = max(1, min(len(sa), len(sb)))  # min-size normalization
    return inter / denom

def pick_best_fact_idx(
    cur_text: str,
    facts: List[Dict[str, Any]],
    mode: str = "loose",
    loose_thresh: float = 0.6
) -> int:
    """
    Return index into `facts` that best matches `cur_text`, or -1 if none.
    facts: [{'fact': str, 'user_knows': bool}, ...]
    """
    if not facts:
        return -1
    text = cur_text.lower()
    best_i, best_score = -1, 0.0
    for i, row in enumerate(facts):
        fact = row.get("fact", "")
        if not fact:
            continue
        if mode == "exact":
            if fact.lower() in text:
                return i
        else:  # loose
            score = _loose_overlap(text, fact)
            if score > best_score:
                best_score, best_i = score, i
    if mode == "loose" and best_score >= loose_thresh:
        return best_i
    return -1


# ---------------- Build prompts/labels (+ knowledge fact idx) ----------------
def build_prompts_labels_metas(
    rows: List[Dict[str, Any]],
    attr: str,
    knowledge_fact_index: bool,
    kfact_scope: str,
    kfact_mode: str,
    kfact_loose_thresh: float,
) -> Tuple[List[str], List[str], List[Tuple[str,int]], Optional[List[int]], Optional[List[List[Dict[str,Any]]]]]:
    """
    Returns:
      prompts, labels, metas, kfact_idx (or None), kfact_tables (or None)
    kfact_idx is aligned with prompts/labels only for attr='knowledge' and knowledge_fact_index=True.
    """
    prompts: List[str] = []
    labels:  List[str] = []
    metas:   List[Tuple[str,int]] = []
    kfact_idx: Optional[List[int]] = [] if (attr == "knowledge" and knowledge_fact_index) else None
    kfact_tables: Optional[List[List[Dict[str,Any]]]] = [] if (attr == "knowledge" and knowledge_fact_index) else None

    conv_auto = 0
    for rec in rows:
        dim = (rec.get("meta", {}).get("dimension") or "").lower()
        if dim != attr:
            continue

        conv_id = str(rec.get("meta", {}).get("dialog_id")
                      or rec.get("meta", {}).get("interaction_id")
                      or rec.get("meta", {}).get("seed")
                      or f"conv_{conv_auto}")
        conv_auto += 1

        dialog = rec.get("dialog", [])
        if not dialog:
            continue

        facts = []
        if attr == "knowledge":
            facts = rec.get("meta", {}).get("knowledge_facts") or []

        # generate one example per USER turn
        user_turn_idx = -1
        running_transcript_parts: List[str] = []
        for k, turn in enumerate(dialog):
            spk = (turn.get("speaker","user") or "user").lower()
            text = turn.get("text","")
            # append this turn to running transcript (for knowledge scope if needed)
            running_transcript_parts.append(f"{(USER_PREFIX if spk=='user' else ASSIST_PREFIX)}{text}")

            if spk != "user":
                continue
            user_turn_idx += 1

            # transcript up to and including this user turn
            base = "\n".join(running_transcript_parts)
            full = append_probe(base, attr)

            # label from THIS user turn
            lab = normalize_turn_label(attr, turn.get("user_attribute_value"))

            prompts.append(full)
            labels.append(lab)
            metas.append((conv_id, user_turn_idx))

            # knowledge fact index (optional)
            if kfact_idx is not None:
                if kfact_scope == "user_turn":
                    scope_text = f"{USER_PREFIX}{text}"
                else:  # transcript
                    scope_text = base
                idx = pick_best_fact_idx(scope_text, facts, mode=kfact_mode, loose_thresh=kfact_loose_thresh)
                kfact_idx.append(int(idx))
                kfact_tables.append(facts)

    return prompts, labels, metas, kfact_idx, kfact_tables


# ---------------- One-pass, all-layers extraction ----------------
@torch.no_grad()
def extract_all_layers_last_token(
    model,
    tokenizer,
    prompts: Sequence[str],
    out_dir: str,
    attr: str,
    device: str,
    batch_size: int,
    max_seq_len: int,
    truncation_side: str,
    save_dtype: str,
    layers: Sequence[int] = None,
    use_device_map_auto: bool = False,
):
    """Single attribute pass: request hidden_states, slice last-token per layer on-GPU, stream to disk."""
    tokenizer.truncation_side = truncation_side

    # probe layer structure
    enc0 = tokenizer(["probe"], return_tensors="pt", padding=True, truncation=True, max_length=8)
    if not use_device_map_auto and device.startswith("cuda"):
        enc0 = {k: v.to(device) for k, v in enc0.items()}
    o0 = model.model(**enc0, use_cache=False, output_hidden_states=True, return_dict=True)
    n_hidden_states = len(o0.hidden_states)  # embeddings + L layers
    L = n_hidden_states - 1
    hidden_size = o0.hidden_states[-1].shape[-1]
    del o0, enc0
    torch.cuda.empty_cache() if device.startswith("cuda") else None

    if layers is None or len(layers) == 0:
        layers = list(range(L))
    for li in layers:
        if li < 0 or li >= L:
            raise ValueError(f"Layer index {li} out of range [0, {L-1}]")

    # prepare per-layer memmaps
    N = len(prompts)
    np_dtype = np.float16 if save_dtype == "float16" else np.float32
    tmp_dir = os.path.join(out_dir, f"__tmp_{attr}")
    ensure_dir(tmp_dir)
    mmaps = {}
    for li in layers:
        mm_path = os.path.join(tmp_dir, f"{attr}_L{li}.mmap")
        mmaps[li] = np.memmap(mm_path, mode="w+", dtype=np_dtype, shape=(N, hidden_size))

    # stream batches
    offset = 0
    for i in tqdm(range(0, N, batch_size), desc=f"Extract {attr} (all layers)"):
        batch = prompts[i:i+batch_size]
        enc = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_seq_len,
        )
        if not use_device_map_auto and device.startswith("cuda"):
            enc = {k: v.to(device) for k, v in enc.items()}

        out = model.model(**enc, use_cache=False, output_hidden_states=True, return_dict=True)
        hs_all = out.hidden_states

        attn = enc["attention_mask"]
        last_idx = torch.clamp(attn.sum(dim=-1) - 1, min=0)  # [B]
        B = attn.size(0)
        arange_b = torch.arange(B, device=last_idx.device)

        for li in layers:
            hs = hs_all[li + 1]                  # skip embeddings
            slice_b = hs[arange_b, last_idx, :]  # [B, H]
            if save_dtype == "float16":
                chunk = slice_b.detach().to("cpu").half().numpy()
            else:
                chunk = slice_b.detach().to("cpu").float().numpy()
            mmaps[li][offset:offset+B, :] = chunk

        offset += B
        del out, hs_all, enc
        torch.cuda.empty_cache() if device.startswith("cuda") else None

    for li in layers:
        mmaps[li].flush()

    return L, hidden_size, layers


# ---------------- CLI ----------------
def parse_layers_arg(s: str) -> List[int]:
    s = s.strip().lower()
    if s == "all":
        return []
    vals = []
    for tok in s.split(","):
        tok = tok.strip()
        if tok:
            vals.append(int(tok))
    return vals

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--data", required=True, help="Path to JSONL produced by generate_llm_user_model_dataset.py")
    p.add_argument("--model", required=True, help="HF model id (e.g., meta-llama/Meta-Llama-3-8B-Instruct)")
    p.add_argument("--attr", choices=ATTRS, required=True, help="Attribute to extract")
    p.add_argument("--out_dir", required=True, help="Directory to write per-layer NPZ files")

    # runtime
    p.add_argument("--device", choices=["cuda","cpu"], default="cuda")
    p.add_argument("--device_map", choices=["auto","none"], default="none")
    p.add_argument("--dtype", choices=["bfloat16","float16","float32"], default="float16")
    p.add_argument("--max_seq_len", type=int, default=2048)
    p.add_argument("--batch_size", type=int, default=4)
    p.add_argument("--truncation_side", choices=["left","right"], default="left")
    p.add_argument("--save_dtype", choices=["float16","float32"], default="float16",
                   help="Disk dtype for X (feature matrix)")
    p.add_argument("--layers", type=str, default="all",
                   help='Comma-separated layer indices (e.g., "0,6,12,18,26,31") or "all"')
    p.add_argument("--save_prompts", action="store_true")

    # knowledge-fact indexing
    p.add_argument("--knowledge_fact_index", choices=["on","off"], default="off",
                   help="If 'on' and attr=knowledge, save per-example fact indices")
    p.add_argument("--kfact_scope", choices=["user_turn","transcript"], default="user_turn",
                   help="Search only the current user turn or the running transcript up to that turn")
    p.add_argument("--kfact_mode", choices=["exact","loose"], default="loose",
                   help="Exact substring match (case-insensitive) or loose token-overlap")
    p.add_argument("--kfact_loose_thresh", type=float, default=0.6,
                   help="Threshold for loose token-overlap (0..1)")

    args = p.parse_args()
    ensure_dir(args.out_dir)

    # load dataset & build per-turn prompts/labels (+ optional kfact)
    rows = load_jsonl(args.data)
    prompts, labels, metas, kfact_idx, kfact_tables = build_prompts_labels_metas(
        rows=rows,
        attr=args.attr,
        knowledge_fact_index=(args.knowledge_fact_index == "on"),
        kfact_scope=args.kfact_scope,
        kfact_mode=args.kfact_mode,
        kfact_loose_thresh=args.kfact_loose_thresh,
    )
    N = len(prompts)
    print(f"[{args.attr}] USER-turn examples: {N}")
    if N == 0:
        print("No examples found for this attribute; nothing to extract.")
        return

    # load model/tokenizer
    torch_dtype = {"bfloat16": torch.bfloat16, "float16": torch.float16, "float32": torch.float32}[args.dtype]
    print(f"[Loading model: {args.model}]")
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    use_device_map_auto = (args.device == "cuda" and args.device_map == "auto")
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch_dtype if args.device == "cuda" else None,
        low_cpu_mem_usage=True,
        device_map=("auto" if use_device_map_auto else None),
    )
    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id
    model.eval()
    if (args.device == "cuda") and not use_device_map_auto:
        model.to("cuda")

    if args.save_prompts:
        path = os.path.join(args.out_dir, f"{args.attr}_prompts.txt")
        with open(path, "w", encoding="utf-8") as f:
            for pmt in prompts:
                f.write(pmt + "\n\n" + ("-"*80) + "\n\n")
        print(f"[Saved prompts] {path}")

    # one pass per attribute â†’ all layers
    requested_layers = parse_layers_arg(args.layers)
    L_total, hidden_size, layers_saved = extract_all_layers_last_token(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        out_dir=args.out_dir,
        attr=args.attr,
        device=args.device,
        batch_size=args.batch_size,
        max_seq_len=args.max_seq_len,
        truncation_side=args.truncation_side,
        save_dtype=args.save_dtype,
        layers=requested_layers,      # [] => all
        use_device_map_auto=use_device_map_auto,
    )

    # finalize: write per-layer npz
    y = np.array(labels, dtype=object)
    metas_np = np.array(metas, dtype=object)
    tmp_dir = os.path.join(args.out_dir, f"__tmp_{args.attr}")

    # knowledge extras
    knowledge_facts_for_info = None
    kfact_idx_np = None
    if (args.attr == "knowledge") and (kfact_idx is not None):
        kfact_idx_np = np.array(kfact_idx, dtype=np.int32)
        # store the fact table from the *first* record (all knowledge dialogs carry their own table;
        # we save per-example indices aligned to that example's own table, so also stash all tables if you want)
        # For simplicity, we serialize NONE here; instead embed the *per-example* tables in info as a list,
        # which can be large. Alternative: store only the first table or skip; we store per-example to be precise.

    for li in layers_saved:
        mm_path = os.path.join(tmp_dir, f"{args.attr}_L{li}.mmap")
        X = np.memmap(mm_path, mode="r", dtype=(np.float16 if args.save_dtype=="float16" else np.float32),
                      shape=(N, hidden_size))
        out_path = os.path.join(args.out_dir, f"{args.attr}_L{li}.npz")

        payload = {
            "X": np.array(X),
            "y": y,
            "metas": metas_np,
            "info": {
                "model": args.model,
                "hidden_size": int(hidden_size),
                "layer_index": int(li),
                "prompt_suffix": PROMPT_SUFFIX_FMT.format(attribute=args.attr),
                "max_seq_len": int(args.max_seq_len),
                "batch_size": int(args.batch_size),
                "save_dtype": args.save_dtype,
            },
        }
        if (args.attr == "knowledge") and (kfact_idx_np is not None):
            payload["kfact_idx"] = kfact_idx_np

        np.savez_compressed(out_path, **payload)
        print(f"[Saved] {out_path}  |  X={(N, hidden_size)}  y={y.shape}")
        del X

    # cleanup
    try:
        for li in layers_saved:
            os.remove(os.path.join(tmp_dir, f"{args.attr}_L{li}.mmap"))
        os.rmdir(tmp_dir)
    except Exception:
        pass

    print(f"[Done] attr={args.attr} | layers={layers_saved} | hidden={hidden_size} | N={N}")


if __name__ == "__main__":
    main()
